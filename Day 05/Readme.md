# **Big Data Fundamentals**  
#### **What is Big Data?**  
Big Data refers to extremely large datasets that are difficult to process using traditional data management tools. The volume and complexity of this data grow exponentially, requiring specialized frameworks like Hadoop.  

---

### **5 V‚Äôs of Big Data**  
1. **Volume** ‚Äì Huge amounts of data are generated, ranging from terabytes to petabytes and exabytes.  
2. **Velocity** ‚Äì Data is generated at high speed (e.g., real-time data from social media and IoT).  
3. **Variety** ‚Äì Different formats of data: structured (databases), semi-structured (JSON, XML), and unstructured (images, videos, emails).  
4. **Veracity** ‚Äì Ensuring the reliability and accuracy of data.  
5. **Value** ‚Äì Extracting meaningful insights to make business decisions.  

---

### **Examples of Big Data**  
- **Social Media** ‚Äì Data from Facebook, Twitter, Instagram.  
- **Healthcare** ‚Äì Patient records, medical images.  
- **Finance** ‚Äì Stock exchange data, transactions.  
- **E-commerce** ‚Äì Purchase data from Amazon, eBay.  
- **IoT** ‚Äì Data from smart devices.  
- **Transportation** ‚Äì Traffic data, GPS logs from Google Maps, Uber.  

---

### **Types of Data**  
1. **Structured Data** ‚Äì Stored in relational databases (e.g., SQL).  
2. **Unstructured Data** ‚Äì Includes emails, images, videos, PDFs.  
3. **Semi-Structured Data** ‚Äì A mix of both structured and unstructured data (e.g., JSON, XML, email with metadata).  

---

### **What is a Cluster?**  
A **cluster** is a collection of multiple computers (nodes) working together to act as a single system. This improves performance, scalability, and fault tolerance.  

---

### **Scaling in Clusters**  
1. **Vertical Scaling (Scaling Up)** ‚Äì Adding more resources (CPU, RAM, storage) to a single machine.  
2. **Horizontal Scaling (Scaling Out)** ‚Äì Adding more machines (nodes) to distribute the load.  

---

### **Hadoop Overview**  
Hadoop is an **open-source framework** designed for **storing and processing** large datasets across multiple computers.  

**Main Components of Hadoop:**  
1. **Hadoop Distributed File System (HDFS)** ‚Äì Storage layer, distributing data across multiple nodes.  
2. **MapReduce** ‚Äì Processing framework that breaks tasks into small chunks and processes them in parallel.  
3. **Yet Another Resource Negotiator (YARN)** ‚Äì Manages cluster resources and schedules jobs efficiently.  

---

### **Properties of Hadoop**  
- **Scalability** ‚Äì Can handle growing data volumes.  
- **Cost-effectiveness** ‚Äì Uses inexpensive hardware.  
- **Flexibility** ‚Äì Works with all types of data.  
- **Fault Tolerance** ‚Äì Replicates data to prevent loss.  
- **Data Locality** ‚Äì Moves computation to the node where data is stored, reducing network congestion.  
- **Simplicity** ‚Äì Uses MapReduce for easy data processing.  
- **Open-source** ‚Äì Free to use and modify.  

---

### **HDFS (Hadoop Distributed File System)**  
HDFS follows a **Master-Slave Architecture** with two key components:  
1. **NameNode (Master)** ‚Äì Stores metadata, tracks file locations.  
2. **DataNode (Slave)** ‚Äì Stores actual data blocks and responds to read/write requests.  

#### **Key HDFS Concepts**  
- **Block Size** ‚Äì Default is **128MB or 256MB**, much larger than traditional file systems.  
- **Replication** ‚Äì Default is **3 copies per block** for fault tolerance.  
- **Rack Awareness** ‚Äì Distributes data across different racks for better fault tolerance.  

---

### **Secondary NameNode vs. Standby NameNode**  
1. **Secondary NameNode** ‚Äì Helps in metadata checkpointing but **does not act as a failover** for NameNode failure.  
2. **Standby NameNode** ‚Äì A **hot backup** that synchronizes with the active NameNode and **automatically takes over in case of failure**.  

---

### **HDFS Operations**  
1. **Write Operation** ‚Äì Data is broken into blocks, replicated, and stored across multiple DataNodes.  
2. **Read Operation** ‚Äì Data is fetched from the nearest DataNode to minimize latency.  

---


<br/>
<br/>

# **HDFS Blocks in Hadoop**  

A **block** in Hadoop is the smallest unit of storage in the **Hadoop Distributed File System (HDFS)**. Unlike traditional file systems (where block sizes are typically **4 KB to 64 KB**), Hadoop uses much larger blocks (default **128 MB or 256 MB**) to efficiently handle **big data processing**.  

---

## **1. Why Does Hadoop Use Large Blocks?**  
- **Reduces Metadata Overhead**: Each file stored in HDFS has metadata maintained by the NameNode. If the block size were small, the NameNode would need to manage too many blocks, leading to high memory usage.  
- **Optimized for Streaming Reads**: Hadoop is designed for processing large datasets, not small, random-access reads. Large blocks ensure **efficient sequential data processing**.  
- **Minimizes Network Traffic**: Since large blocks reduce the number of splits, **less data is transferred between nodes**, improving performance.  

---

## **2. HDFS Block Characteristics**  
- **Default Block Size**: **128 MB** (Hadoop 2.x and later) or **64 MB** (Hadoop 1.x). Can be configured based on workload.  
- **Logical Representation**: Unlike physical disk blocks, HDFS blocks are **logical**‚Äîa single block **can span multiple physical disks** across different DataNodes.  
- **File Splitting**: If a file is larger than the block size, it is split into **multiple blocks** (e.g., a **500 MB file** will be split into **four 128 MB blocks + one 4 MB block**).  
- **No Wastage of Space**: If a block **is not fully utilized**, the remaining space **is not shared** with other files (e.g., a 50 MB file in a 128 MB block still uses one full block).  

---

## **3. Example: How Data is Stored in HDFS Blocks**  
Let's assume:  
- Block size = **128 MB**  
- Replication factor = **3**  

| File Size  | Blocks Created |
|------------|--------------|
| 200 MB  | Block 1 (128 MB) + Block 2 (72 MB) |
| 300 MB  | Block 1 (128 MB) + Block 2 (128 MB) + Block 3 (44 MB) |

Each block is **replicated three times** across different DataNodes for **fault tolerance**.  

---

## **4. Block Replication (Fault Tolerance)**  
HDFS ensures **high availability** by replicating blocks across multiple DataNodes.  
- **Default Replication Factor**: **3 copies**  
- **Placement Strategy**:  
  - First copy ‚Üí Stored on a **random node in the cluster**  
  - Second copy ‚Üí Stored on a **different node in a different rack**  
  - Third copy ‚Üí Stored on **another node in the same rack as the second copy**  

If a node fails, Hadoop automatically **re-replicates lost blocks** from existing copies to maintain the replication factor.  

---

## **5. Why is HDFS Block Size Larger than Traditional File Systems?**  
| Feature           | Traditional File Systems | HDFS |
|------------------|------------------------|------|
| Block Size       | 4 KB ‚Äì 64 KB             | 128 MB ‚Äì 256 MB |
| Storage Type     | Local disks              | Distributed across clusters |
| Read/Write Mode  | Random Access            | Sequential Processing |
| Performance      | Fast for small files     | Optimized for large files |

### **Key Takeaway**  
HDFS uses **large blocks** to handle **big data efficiently**, minimize **metadata overhead**, and support **fault tolerance through replication**.

---

<br/>
<br/>

# **üìå Secondary NameNode vs. Standby NameNode in Hadoop**  

In Hadoop, **both the Secondary NameNode and Standby NameNode** play a role in managing the **NameNode‚Äôs metadata**, but their functions are **very different**. Many people mistakenly assume that the **Secondary NameNode is a backup NameNode**, but this is **incorrect**.

---

## **üü¢ 1. What is Secondary NameNode?**  

‚úÖ The **Secondary NameNode (SNN)** is **NOT a failover NameNode**.  
‚úÖ Its primary job is to **periodically merge the edit logs** with the **fsimage** to create an updated snapshot of the metadata.  
‚úÖ It helps in **reducing the edit log size** so that the NameNode can restart faster.  

### **üîπ How Does Secondary NameNode Work?**  
1Ô∏è‚É£ **Copies fsimage and edit logs from NameNode**.  
2Ô∏è‚É£ **Applies edit logs to the fsimage** (i.e., merges changes).  
3Ô∏è‚É£ **Saves the new fsimage back to NameNode**.  
4Ô∏è‚É£ **Deletes old edit logs** after merging to reduce memory usage.  

üìå **Understanding:**  
üí° *Think of the Secondary NameNode as a "metadata assistant" that cleans up logs to ensure the NameNode starts faster.*  

---

## **üü¢ 2. What is Standby NameNode?**  

‚úÖ The **Standby NameNode (SBN)** is a **hot backup** that takes over immediately if the **Active NameNode fails**.  
‚úÖ It runs in a **high-availability (HA) Hadoop cluster**.  
‚úÖ It constantly **syncs with the Active NameNode** using **journal nodes** to keep its metadata up to date.  

### **üîπ How Does Standby NameNode Work?**  
1Ô∏è‚É£ **Continuously receives metadata updates** from Active NameNode.  
2Ô∏è‚É£ **Uses JournalNodes to keep itself in sync** with Active NameNode.  
3Ô∏è‚É£ If Active NameNode **fails**, the Standby NameNode **automatically takes over**.  

üìå **Understanding:**  
üí° *The Standby NameNode is a "live duplicate" of the Active NameNode and ensures high availability of the Hadoop cluster.*  

---

## **üü¢ 3. Key Differences Between Secondary NameNode and Standby NameNode**  

| **Feature** | **Secondary NameNode** | **Standby NameNode** |
|------------|----------------|----------------|
| **Purpose** | Merges edit logs to create a clean `fsimage`. | Acts as a **backup** for failover in HA mode. |
| **Failover Support** | ‚ùå **NOT a failover** NameNode. | ‚úÖ **Supports automatic failover**. |
| **Real-Time Sync** | ‚ùå Periodic sync only. | ‚úÖ **Continuously syncs** with Active NameNode. |
| **Journal Nodes** | ‚ùå Not used. | ‚úÖ Uses **JournalNodes** for synchronization. |
| **Cluster Type** | Works in **non-HA Hadoop clusters**. | Works in **HA-enabled clusters**. |
| **Metadata Storage** | Stores a **backup copy of fsimage** but doesn‚Äôt take over. | Stores **live metadata and can take over** instantly. |
| **Data Recovery** | Helps in **recovering metadata** if NameNode restarts. | **Takes over instantly** if Active NameNode crashes. |

üìå **Understanding:**  
üí° *The Secondary NameNode only helps with metadata cleanup, while the Standby NameNode ensures high availability by acting as a live backup.*  

---

## **üü¢ 4. When to Use Secondary NameNode vs. Standby NameNode?**  

| **Use Case** | **Solution** |
|-------------|-------------|
| **Small clusters** where failover is not critical. | Use **Secondary NameNode** (manual restart needed if NameNode fails). |
| **Large production clusters** where downtime is unacceptable. | Use **Standby NameNode** (automatic failover). |

üìå **Understanding:**  
üí° *For enterprise-level production environments, always use a **Standby NameNode** for high availability.*  

---

## **üü¢ 5. High Availability (HA) Configuration with Standby NameNode**  

To enable **HA mode** in Hadoop:  
1Ô∏è‚É£ **Set up two NameNodes** ‚Äì One **Active** and one **Standby**.  
2Ô∏è‚É£ **Use JournalNodes** to sync metadata between them.  
3Ô∏è‚É£ **Configure Zookeeper** to monitor NameNode health and trigger failover if needed.  

üìå **Understanding:**  
üí° *Without HA mode, if a NameNode crashes, the entire cluster stops working. Standby NameNode prevents this by taking over instantly.*  

---

## **‚úÖ Summary: Key Takeaways**  
üìå **Secondary NameNode is NOT a backup** ‚Äì It only **merges metadata** to speed up NameNode recovery.  
üìå **Standby NameNode is a failover NameNode** ‚Äì It takes over automatically when Active NameNode fails.  
üìå **Use Standby NameNode in production** for **high availability**.  
üìå **JournalNodes + Zookeeper** ensure real-time failover in an HA-enabled cluster.  

<br/>
<br/>


## **üìå Explanation of HDFS HA Architecture (with Automatic Failover and QuorumJournalManager)**  
![alt text](image.png)
This diagram represents the **High Availability (HA) Architecture** of **HDFS (Hadoop Distributed File System)** with **automatic failover** using **Quorum Journal Manager (QJM)**.

---

## **üü¢ Key Components in the Diagram**  

### **1Ô∏è‚É£ NameNodes (NN)**  
- **NN Active (Active NameNode)**:  
  - Handles all client requests for file operations.  
  - Maintains the latest metadata of HDFS.  
  - Communicates with DataNodes (DNs).  
- **NN Standby (Standby NameNode)**:  
  - Synchronizes with the Active NameNode.  
  - Remains in a "hot backup" state.  
  - Takes over automatically if the Active NN fails.  

---

### **2Ô∏è‚É£ DataNodes (DNs)**  
- These are the **worker nodes** in Hadoop.  
- They store the **actual data blocks** of HDFS files.  
- Both the Active and Standby NameNodes receive block reports from DataNodes.  

---

### **3Ô∏è‚É£ JournalNodes (JN)**  
- These are the **Quorum Journal Nodes** (at least **3** nodes for fault tolerance).  
- They **store metadata updates** from the Active NameNode.  
- The Standby NameNode **reads from the JournalNodes** to stay synchronized.  

üìå **Understanding:**  
üí° *The JournalNodes ensure that the Standby NameNode has the latest metadata in case of failover.*  

---

### **4Ô∏è‚É£ Zookeeper (ZK) and Failover Controllers**  
- **Zookeeper (ZK)** monitors the health of the NameNodes.  
- **Failover Controller (FC)**:  
  - One is assigned as **Active Failover Controller** and another as **Standby Failover Controller**.  
  - It keeps track of the health of **Active and Standby NameNodes**.  
  - Automatically switches the Active NameNode to the Standby NameNode if needed.  

üìå **Understanding:**  
üí° *Zookeeper + Failover Controller helps in automatic failover if the Active NameNode crashes.*  

---

## **üü¢ Working of HDFS HA with Automatic Failover**  

1Ô∏è‚É£ **Normal Operation:**  
- The **Active NameNode** processes all client requests.  
- The **Standby NameNode** continuously syncs metadata from JournalNodes.  
- DataNodes send **block reports** to both NameNodes.  

2Ô∏è‚É£ **Metadata Synchronization:**  
- The **Active NN writes edits (metadata changes) to JournalNodes**.  
- The **Standby NN reads from JournalNodes** to stay updated.  

3Ô∏è‚É£ **Automatic Failover in Case of Failure:**  
- **Zookeeper detects a failure** in the Active NameNode.  
- The **Failover Controller triggers a switch** from Active NN to Standby NN.  
- The **Standby NameNode becomes Active** and takes over operations.  

üìå **Understanding:**  
üí° *This process ensures that Hadoop continues to function even if the Active NameNode crashes, preventing downtime.*  

---

## **üü¢ Key Advantages of This HA Architecture**  

‚úÖ **High Availability** ‚Äì Prevents Hadoop from crashing due to NameNode failure.  
‚úÖ **Automatic Failover** ‚Äì No manual intervention needed during failure.  
‚úÖ **Quorum-Based Decision Making** ‚Äì Prevents split-brain scenarios (both NNs becoming active).  
‚úÖ **Better Fault Tolerance** ‚Äì JournalNodes ensure metadata is always available.  

---

## **‚úÖ Summary: How It Works Step-by-Step**  

üîπ **Active NameNode processes client requests.**  
üîπ **All metadata changes are written to JournalNodes.**  
üîπ **Standby NameNode reads from JournalNodes to stay updated.**  
üîπ **Zookeeper and Failover Controllers monitor NameNode health.**  
üîπ **If Active NN fails, Standby NN takes over automatically.**  
üîπ **The system continues without downtime.**  


# **üìå Detailed Explanation of Hadoop Day 2 Notes (MapReduce & YARN)**  

---

## **üü¢ 1. What is MapReduce?**  
**MapReduce** is a **programming model** used in Hadoop to process **large datasets in parallel** across multiple machines in a distributed cluster. It follows a **Divide and Conquer** approach to break large data into smaller chunks and process them efficiently.

### **üîπ How MapReduce Works?**  
MapReduce consists of two main phases:  

1. **Map Phase (Mapper Job)**
   - Reads a **block of data** and **processes** it into **key-value pairs**.
   - The key-value pairs are **intermediate outputs**.

2. **Reduce Phase (Reducer Job)**
   - Takes intermediate key-value pairs from **multiple mappers**.
   - Aggregates, summarizes, and processes them into the **final output**.

üëâ **Example:** *Word Count Problem*  
- **Input:** `"Hadoop is great. Hadoop is fast."`
- **Mapper Output (Intermediate Key-Value Pairs):**  
  ```json
  ("Hadoop", 1), ("is", 1), ("great.", 1), ("Hadoop", 1), ("is", 1), ("fast.", 1)
  ```
- **Reducer Output (Final Aggregated Result):**  
  ```json
  ("Hadoop", 2), ("is", 2), ("great.", 1), ("fast.", 1)
  ```

---

## **üü¢ 2. Advantages of MapReduce**
‚úÖ **Parallel Processing** ‚Äì Jobs run on multiple nodes, reducing execution time.  
‚úÖ **Data Locality** ‚Äì Instead of moving large data to computation, MapReduce moves computation to where data is stored.  
‚úÖ **Scalability** ‚Äì Handles petabytes of data.  
‚úÖ **Fault Tolerance** ‚Äì If a node fails, tasks are re-executed on another node.  

---

## **üü¢ 3. MapReduce Workflow (Data Flow)**  

### **üîπ Input Files & Input Splitting**  
üìå **Input Data is stored in HDFS** and split into smaller chunks called **InputSplits**. Each split is assigned to a Mapper.  
- **Example:** A 1 GB file with 128 MB block size ‚Üí **8 blocks** ‚Üí **8 InputSplits** ‚Üí **8 Mappers**.  

### **üîπ Key Components of MapReduce Execution**  

| **Component**  | **Description** |
|---------------|---------------|
| **InputFormat** | Defines how input data is split and read (e.g., TextInputFormat, SequenceFileInputFormat). |
| **RecordReader** | Converts raw input into key-value pairs (e.g., `(line number, line content)`). |
| **Mapper** | Processes input and generates intermediate key-value pairs. |
| **Combiner (Mini Reducer)** | Performs **local aggregation** of key-value pairs (reduces data before sending to Reducer). |
| **Partitioner** | Distributes key-value pairs among Reducers based on the key. |
| **Shuffling & Sorting** | Groups key-value pairs with the same key before reducing. |
| **Reducer** | Aggregates and produces final output. |
| **RecordWriter** | Writes final output to HDFS. |
| **OutputFormat** | Defines how final output is written (e.g., TextOutputFormat). |

---

## **üü¢ 4. Difference Between Input Split & Block**  
| **Feature**  | **Input Split**  | **HDFS Block**  |
|------------|--------------|--------------|
| **Definition** | Logical division of data for processing | Physical division of data for storage |
| **Size** | Variable (depends on data & split logic) | Fixed (default **128 MB** or **256 MB**) |
| **Used By** | Mapper tasks | HDFS storage system |
| **Processing** | Each split is processed by a **single Mapper** | A block can be used by multiple Mappers |

### **Example:**  
A 500 MB file with a 128 MB block size:  
- **Blocks:** **4 blocks + 1 partial block** (128MB each).  
- **InputSplits:** If we use **8 Mappers**, there will be **8 InputSplits**, even though there are only **5 blocks**.  

---

## **üü¢ 5. What is YARN? (Yet Another Resource Negotiator)**  
YARN is the **resource management layer** in Hadoop. It was introduced in **Hadoop 2.x** to improve **scalability and resource allocation**.  

**Before YARN:**  
- Hadoop 1.0 had a **single JobTracker** (handled scheduling + monitoring), which was a **bottleneck**.  

**After YARN:**  
- Introduced **Resource Manager** (RM) and **Node Managers** (NM) to handle resources separately.  

---

## **üü¢ 6. Components of YARN**  

### **1Ô∏è‚É£ Resource Manager (RM)**
**Master component** that **allocates cluster resources** to applications.  
It has two parts:  
- **Scheduler** ‚Äì Allocates resources to tasks but doesn‚Äôt monitor them.  
- **Application Manager** ‚Äì Manages application lifecycle (starting/stopping jobs).  

### **2Ô∏è‚É£ Node Manager (NM)**
**Runs on each slave node** and reports to the RM.  
- Monitors CPU & memory usage.  
- Launches containers and manages execution.  
- Can kill a task if instructed by RM.  

### **3Ô∏è‚É£ Application Master (AM)**
- Created for each job to handle execution.  
- Requests resources from the RM.  
- Monitors progress and handles **fault tolerance**.  

### **4Ô∏è‚É£ Container**
- **Unit of resource allocation** (CPU, memory).  
- Runs a single **Map or Reduce task**.  

---

## **üü¢ 7. Running an Application through YARN**  

### **Step-by-Step Execution Flow**  

| **Step**  | **Description**  |
|-----------|---------------|
| **1. Job Submission** | The client submits the application to the **Resource Manager (RM)**. |
| **2. Application ID Assignment** | RM assigns a unique **Application ID**. |
| **3. Copy Job Resources** | HDFS stores input data, JAR files, configuration files. |
| **4. Application Submission** | RM assigns an **Application Master (AM)** for execution. |
| **5. Launch Application Master** | AM starts in the first container allocated. |
| **6. Initialize Job** | AM retrieves **input splits** and prepares for execution. |
| **7. Allocate Resources** | AM requests CPU/memory from RM. |
| **8. Start Containers** | Node Managers launch containers and start tasks. |
| **9. Task Execution** | Tasks are executed inside the containers. |
| **10. Monitoring & Fault Tolerance** | AM tracks progress and re-runs failed tasks. |
| **11. Completion & Resource Release** | After job completion, containers and AM are shut down. |

---

## **üü¢ 8. Why YARN is Better than Hadoop 1.0?**  
| **Feature**  | **Hadoop 1.0 (MRv1)** | **Hadoop 2.0 (YARN)** |
|------------|----------------|----------------|
| **Job Scheduling** | **JobTracker** does everything (overloaded) | **Resource Manager + Application Master** |
| **Scalability** | Limited to **40,000 nodes** | Can scale to **1 million nodes** |
| **Resource Utilization** | Jobs wait if TaskTrackers are full | Dynamic allocation of containers |
| **Fault Tolerance** | If JobTracker fails, job restarts | **Automatic re-execution of failed jobs** |

---

## **‚úÖ Summary**  
üìå **MapReduce** efficiently processes large datasets using the **Map ‚Üí Shuffle ‚Üí Reduce** model.  
üìå **YARN** introduced better resource management for **scalability & performance**.  
üìå **HDFS Block vs. Input Split**: Blocks are for **storage**, Splits are for **processing**.  
üìå **Key YARN Components**: **Resource Manager, Node Manager, Application Master, Containers**.  
