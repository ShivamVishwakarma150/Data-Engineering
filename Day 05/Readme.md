# **Big Data Fundamentals**  
#### **What is Big Data?**  
Big Data refers to extremely large datasets that are difficult to process using traditional data management tools. The volume and complexity of this data grow exponentially, requiring specialized frameworks like Hadoop.  

---

### **5 Vâ€™s of Big Data**  
1. **Volume** â€“ Huge amounts of data are generated, ranging from terabytes to petabytes and exabytes.  
2. **Velocity** â€“ Data is generated at high speed (e.g., real-time data from social media and IoT).  
3. **Variety** â€“ Different formats of data: structured (databases), semi-structured (JSON, XML), and unstructured (images, videos, emails).  
4. **Veracity** â€“ Ensuring the reliability and accuracy of data.  
5. **Value** â€“ Extracting meaningful insights to make business decisions.  

---

### **Examples of Big Data**  
- **Social Media** â€“ Data from Facebook, Twitter, Instagram.  
- **Healthcare** â€“ Patient records, medical images.  
- **Finance** â€“ Stock exchange data, transactions.  
- **E-commerce** â€“ Purchase data from Amazon, eBay.  
- **IoT** â€“ Data from smart devices.  
- **Transportation** â€“ Traffic data, GPS logs from Google Maps, Uber.  

---

### **Types of Data**  
1. **Structured Data** â€“ Stored in relational databases (e.g., SQL).  
2. **Unstructured Data** â€“ Includes emails, images, videos, PDFs.  
3. **Semi-Structured Data** â€“ A mix of both structured and unstructured data (e.g., JSON, XML, email with metadata).  

---

### **What is a Cluster?**  
A **cluster** is a collection of multiple computers (nodes) working together to act as a single system. This improves performance, scalability, and fault tolerance.  

---

### **Scaling in Clusters**  
1. **Vertical Scaling (Scaling Up)** â€“ Adding more resources (CPU, RAM, storage) to a single machine.  
2. **Horizontal Scaling (Scaling Out)** â€“ Adding more machines (nodes) to distribute the load.  

---

### **Hadoop Overview**  
Hadoop is an **open-source framework** designed for **storing and processing** large datasets across multiple computers.  

**Main Components of Hadoop:**  
1. **Hadoop Distributed File System (HDFS)** â€“ Storage layer, distributing data across multiple nodes.  
2. **MapReduce** â€“ Processing framework that breaks tasks into small chunks and processes them in parallel.  
3. **Yet Another Resource Negotiator (YARN)** â€“ Manages cluster resources and schedules jobs efficiently.  

---

### **Properties of Hadoop**  
- **Scalability** â€“ Can handle growing data volumes.  
- **Cost-effectiveness** â€“ Uses inexpensive hardware.  
- **Flexibility** â€“ Works with all types of data.  
- **Fault Tolerance** â€“ Replicates data to prevent loss.  
- **Data Locality** â€“ Moves computation to the node where data is stored, reducing network congestion.  
- **Simplicity** â€“ Uses MapReduce for easy data processing.  
- **Open-source** â€“ Free to use and modify.  

---

### **HDFS (Hadoop Distributed File System)**  
HDFS follows a **Master-Slave Architecture** with two key components:  
1. **NameNode (Master)** â€“ Stores metadata, tracks file locations.  
2. **DataNode (Slave)** â€“ Stores actual data blocks and responds to read/write requests.  

#### **Key HDFS Concepts**  
- **Block Size** â€“ Default is **128MB or 256MB**, much larger than traditional file systems.  
- **Replication** â€“ Default is **3 copies per block** for fault tolerance.  
- **Rack Awareness** â€“ Distributes data across different racks for better fault tolerance.  

---

### **Secondary NameNode vs. Standby NameNode**  
1. **Secondary NameNode** â€“ Helps in metadata checkpointing but **does not act as a failover** for NameNode failure.  
2. **Standby NameNode** â€“ A **hot backup** that synchronizes with the active NameNode and **automatically takes over in case of failure**.  

---

### **HDFS Operations**  
1. **Write Operation** â€“ Data is broken into blocks, replicated, and stored across multiple DataNodes.  
2. **Read Operation** â€“ Data is fetched from the nearest DataNode to minimize latency.  

---


<br/>
<br/>

# **HDFS Blocks in Hadoop**  

A **block** in Hadoop is the smallest unit of storage in the **Hadoop Distributed File System (HDFS)**. Unlike traditional file systems (where block sizes are typically **4 KB to 64 KB**), Hadoop uses much larger blocks (default **128 MB or 256 MB**) to efficiently handle **big data processing**.  

---

## **1. Why Does Hadoop Use Large Blocks?**  
- **Reduces Metadata Overhead**: Each file stored in HDFS has metadata maintained by the NameNode. If the block size were small, the NameNode would need to manage too many blocks, leading to high memory usage.  
- **Optimized for Streaming Reads**: Hadoop is designed for processing large datasets, not small, random-access reads. Large blocks ensure **efficient sequential data processing**.  
- **Minimizes Network Traffic**: Since large blocks reduce the number of splits, **less data is transferred between nodes**, improving performance.  

---

## **2. HDFS Block Characteristics**  
- **Default Block Size**: **128 MB** (Hadoop 2.x and later) or **64 MB** (Hadoop 1.x). Can be configured based on workload.  
- **Logical Representation**: Unlike physical disk blocks, HDFS blocks are **logical**â€”a single block **can span multiple physical disks** across different DataNodes.  
- **File Splitting**: If a file is larger than the block size, it is split into **multiple blocks** (e.g., a **500 MB file** will be split into **four 128 MB blocks + one 4 MB block**).  
- **No Wastage of Space**: If a block **is not fully utilized**, the remaining space **is not shared** with other files (e.g., a 50 MB file in a 128 MB block still uses one full block).  

---

## **3. Example: How Data is Stored in HDFS Blocks**  
Let's assume:  
- Block size = **128 MB**  
- Replication factor = **3**  

| File Size  | Blocks Created |
|------------|--------------|
| 200 MB  | Block 1 (128 MB) + Block 2 (72 MB) |
| 300 MB  | Block 1 (128 MB) + Block 2 (128 MB) + Block 3 (44 MB) |

Each block is **replicated three times** across different DataNodes for **fault tolerance**.  

---

## **4. Block Replication (Fault Tolerance)**  
HDFS ensures **high availability** by replicating blocks across multiple DataNodes.  
- **Default Replication Factor**: **3 copies**  
- **Placement Strategy**:  
  - First copy â†’ Stored on a **random node in the cluster**  
  - Second copy â†’ Stored on a **different node in a different rack**  
  - Third copy â†’ Stored on **another node in the same rack as the second copy**  

If a node fails, Hadoop automatically **re-replicates lost blocks** from existing copies to maintain the replication factor.  

---

## **5. Why is HDFS Block Size Larger than Traditional File Systems?**  
| Feature           | Traditional File Systems | HDFS |
|------------------|------------------------|------|
| Block Size       | 4 KB â€“ 64 KB             | 128 MB â€“ 256 MB |
| Storage Type     | Local disks              | Distributed across clusters |
| Read/Write Mode  | Random Access            | Sequential Processing |
| Performance      | Fast for small files     | Optimized for large files |

### **Key Takeaway**  
HDFS uses **large blocks** to handle **big data efficiently**, minimize **metadata overhead**, and support **fault tolerance through replication**.

---

<br/>
<br/>

# **ğŸ“Œ Secondary NameNode vs. Standby NameNode in Hadoop**  

In Hadoop, **both the Secondary NameNode and Standby NameNode** play a role in managing the **NameNodeâ€™s metadata**, but their functions are **very different**. Many people mistakenly assume that the **Secondary NameNode is a backup NameNode**, but this is **incorrect**.

---

## **ğŸŸ¢ 1. What is Secondary NameNode?**  

âœ… The **Secondary NameNode (SNN)** is **NOT a failover NameNode**.  
âœ… Its primary job is to **periodically merge the edit logs** with the **fsimage** to create an updated snapshot of the metadata.  
âœ… It helps in **reducing the edit log size** so that the NameNode can restart faster.  

### **ğŸ”¹ How Does Secondary NameNode Work?**  
1ï¸âƒ£ **Copies fsimage and edit logs from NameNode**.  
2ï¸âƒ£ **Applies edit logs to the fsimage** (i.e., merges changes).  
3ï¸âƒ£ **Saves the new fsimage back to NameNode**.  
4ï¸âƒ£ **Deletes old edit logs** after merging to reduce memory usage.  

ğŸ“Œ **Understanding:**  
ğŸ’¡ *Think of the Secondary NameNode as a "metadata assistant" that cleans up logs to ensure the NameNode starts faster.*  

---

## **ğŸŸ¢ 2. What is Standby NameNode?**  

âœ… The **Standby NameNode (SBN)** is a **hot backup** that takes over immediately if the **Active NameNode fails**.  
âœ… It runs in a **high-availability (HA) Hadoop cluster**.  
âœ… It constantly **syncs with the Active NameNode** using **journal nodes** to keep its metadata up to date.  

### **ğŸ”¹ How Does Standby NameNode Work?**  
1ï¸âƒ£ **Continuously receives metadata updates** from Active NameNode.  
2ï¸âƒ£ **Uses JournalNodes to keep itself in sync** with Active NameNode.  
3ï¸âƒ£ If Active NameNode **fails**, the Standby NameNode **automatically takes over**.  

ğŸ“Œ **Understanding:**  
ğŸ’¡ *The Standby NameNode is a "live duplicate" of the Active NameNode and ensures high availability of the Hadoop cluster.*  

---

## **ğŸŸ¢ 3. Key Differences Between Secondary NameNode and Standby NameNode**  

| **Feature** | **Secondary NameNode** | **Standby NameNode** |
|------------|----------------|----------------|
| **Purpose** | Merges edit logs to create a clean `fsimage`. | Acts as a **backup** for failover in HA mode. |
| **Failover Support** | âŒ **NOT a failover** NameNode. | âœ… **Supports automatic failover**. |
| **Real-Time Sync** | âŒ Periodic sync only. | âœ… **Continuously syncs** with Active NameNode. |
| **Journal Nodes** | âŒ Not used. | âœ… Uses **JournalNodes** for synchronization. |
| **Cluster Type** | Works in **non-HA Hadoop clusters**. | Works in **HA-enabled clusters**. |
| **Metadata Storage** | Stores a **backup copy of fsimage** but doesnâ€™t take over. | Stores **live metadata and can take over** instantly. |
| **Data Recovery** | Helps in **recovering metadata** if NameNode restarts. | **Takes over instantly** if Active NameNode crashes. |

ğŸ“Œ **Understanding:**  
ğŸ’¡ *The Secondary NameNode only helps with metadata cleanup, while the Standby NameNode ensures high availability by acting as a live backup.*  

---

## **ğŸŸ¢ 4. When to Use Secondary NameNode vs. Standby NameNode?**  

| **Use Case** | **Solution** |
|-------------|-------------|
| **Small clusters** where failover is not critical. | Use **Secondary NameNode** (manual restart needed if NameNode fails). |
| **Large production clusters** where downtime is unacceptable. | Use **Standby NameNode** (automatic failover). |

ğŸ“Œ **Understanding:**  
ğŸ’¡ *For enterprise-level production environments, always use a **Standby NameNode** for high availability.*  

---

## **ğŸŸ¢ 5. High Availability (HA) Configuration with Standby NameNode**  

To enable **HA mode** in Hadoop:  
1ï¸âƒ£ **Set up two NameNodes** â€“ One **Active** and one **Standby**.  
2ï¸âƒ£ **Use JournalNodes** to sync metadata between them.  
3ï¸âƒ£ **Configure Zookeeper** to monitor NameNode health and trigger failover if needed.  

ğŸ“Œ **Understanding:**  
ğŸ’¡ *Without HA mode, if a NameNode crashes, the entire cluster stops working. Standby NameNode prevents this by taking over instantly.*  

---

## **âœ… Summary: Key Takeaways**  
ğŸ“Œ **Secondary NameNode is NOT a backup** â€“ It only **merges metadata** to speed up NameNode recovery.  
ğŸ“Œ **Standby NameNode is a failover NameNode** â€“ It takes over automatically when Active NameNode fails.  
ğŸ“Œ **Use Standby NameNode in production** for **high availability**.  
ğŸ“Œ **JournalNodes + Zookeeper** ensure real-time failover in an HA-enabled cluster.  

ğŸš€ **Want to set up HA mode in Hadoop?** Let me know if you need **configuration steps!** ğŸ˜Š